# ğŸ” NSFW / Nudity Detector (Image Moderation)

This module automatically detects and blocks unsafe or inappropriate images (e.g., nudity, gore, weapons) before they are allowed on your clothing exchange platform like **ReWear**.

---

## âœ… Features

- ğŸ” Detects **nudity**: raw, partial, or safe
- ğŸ”« Flags **weapons**: guns, knives, etc.
- âš ï¸ Detects **offensive content**: symbols, gestures, hate content
- ğŸ’‰ Filters **drugs** and **alcohol**
- ğŸ©¸ Detects **gore** and **violence**
- ğŸ”¤ Analyzes **text in images** for profanity, extremism, or spam
- ğŸš« Automatically blocks unsafe images
- ğŸ“¦ Easy integration into Python or Flask backend

---

## ğŸ› ï¸ How It Works

1. User uploads an image
2. The image is sent to **Sightengineâ€™s Moderation API**
3. The API returns a structured response with content probabilities
4. If unsafe content is detected, the image is blocked

---

## Image used when image is safe:
![1739042759287](https://github.com/user-attachments/assets/c6100fdb-8c6c-4e3d-b6d6-b1115d969106)
